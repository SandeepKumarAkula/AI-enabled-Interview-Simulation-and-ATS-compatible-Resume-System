/**
 * REAL-TIME VIDEO ANALYSIS ENGINE
 * 
 * Uses MediaPipe and TensorFlow.js for actual computer vision analysis:
 * - Face detection and landmark tracking (468 points)
 * - Gaze estimation (eye contact detection)
 * - Pose detection (body posture analysis)
 * - Expression classification
 * - Movement stability tracking
 * 
 * All processing happens in browser - no frames sent to server
 */

import { FaceLandmarker, PoseLandmarker, FilesetResolver } from "@mediapipe/tasks-vision"

export interface VideoAnalysisResult {
  eyeContactScore: number // 0-100: How much candidate looks at camera
  postureScore: number // 0-100: Upright vs slouching
  composureScore: number // 0-100: Calm vs fidgeting
  engagementScore: number // 0-100: Overall presence
  facialExpression: "confident" | "nervous" | "neutral" | "uncertain"
  gazeDirection: "center" | "left" | "right" | "down" | "up"
  bodyPosture: "upright" | "slouching" | "leaning"
  movementStability: number // 0-100: Still vs fidgety
  timestamp: number
}

class VideoAnalyzer {
  private faceLandmarker: FaceLandmarker | null = null
  private poseLandmarker: PoseLandmarker | null = null
  private initialized = false
  private lastAnalysis: VideoAnalysisResult | null = null
  private analysisHistory: VideoAnalysisResult[] = []
  
  // Calibration: Track initial pose as "neutral"
  private calibrationFrames: { faceCenter: [number, number], shoulderY: number }[] = []
  private isCalibrated = false
  private neutralFaceCenter: [number, number] = [0.5, 0.5]
  private neutralShoulderY = 0.5

  async initialize() {
    if (this.initialized) return

    try {
      console.log("ðŸ”§ Initializing MediaPipe models...")
      
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
      )

      // Initialize Face Landmarker (for eye contact and expression)
      this.faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath: "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
          delegate: "GPU"
        },
        outputFaceBlendshapes: true,
        outputFacialTransformationMatrixes: true,
        runningMode: "VIDEO",
        numFaces: 1
      })

      // Initialize Pose Landmarker (for body posture)
      this.poseLandmarker = await PoseLandmarker.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath: "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task",
          delegate: "GPU"
        },
        runningMode: "VIDEO",
        numPoses: 1
      })

      this.initialized = true
      console.log("âœ… Video analysis models loaded")
    } catch (error) {
      console.error("âŒ Failed to initialize video analysis:", error)
      throw error
    }
  }

  /**
   * Calibrate by analyzing first few frames to establish neutral position
   */
  calibrate(videoElement: HTMLVideoElement) {
    if (!this.faceLandmarker || !this.poseLandmarker) return

    try {
      const faceResults = this.faceLandmarker.detectForVideo(videoElement, Date.now())
      const poseResults = this.poseLandmarker.detectForVideo(videoElement, Date.now())

      if (faceResults.faceLandmarks.length > 0 && poseResults.landmarks.length > 0) {
        const face = faceResults.faceLandmarks[0]
        const pose = poseResults.landmarks[0]
        
        // Face center (average of key points)
        const noseTip = face[1] // Nose tip landmark
        const faceCenter: [number, number] = [noseTip.x, noseTip.y]
        
        // Shoulder Y position (average of left and right shoulders)
        const leftShoulder = pose[11]
        const rightShoulder = pose[12]
        const shoulderY = (leftShoulder.y + rightShoulder.y) / 2

        this.calibrationFrames.push({ faceCenter, shoulderY })

        // After 10 frames, calculate neutral position
        if (this.calibrationFrames.length >= 10) {
          this.neutralFaceCenter = [
            this.calibrationFrames.reduce((sum, f) => sum + f.faceCenter[0], 0) / 10,
            this.calibrationFrames.reduce((sum, f) => sum + f.faceCenter[1], 0) / 10
          ]
          this.neutralShoulderY = this.calibrationFrames.reduce((sum, f) => sum + f.shoulderY, 0) / 10
          this.isCalibrated = true
          console.log("âœ… Video analysis calibrated", { neutralFaceCenter: this.neutralFaceCenter, neutralShoulderY: this.neutralShoulderY })
        }
      }
    } catch (error) {
      console.error("Calibration error:", error)
    }
  }

  /**
   * Analyze a video frame for non-verbal cues
   */
  async analyzeFrame(videoElement: HTMLVideoElement): Promise<VideoAnalysisResult | null> {
    if (!this.initialized || !this.faceLandmarker || !this.poseLandmarker) {
      console.warn("Video analyzer not initialized")
      return null
    }

    // Calibration phase
    if (!this.isCalibrated) {
      this.calibrate(videoElement)
      return null
    }

    try {
      const timestamp = Date.now()
      
      // Run MediaPipe detection
      const faceResults = this.faceLandmarker.detectForVideo(videoElement, timestamp)
      const poseResults = this.poseLandmarker.detectForVideo(videoElement, timestamp)

      if (faceResults.faceLandmarks.length === 0 || poseResults.landmarks.length === 0) {
        // No face/pose detected
        return {
          eyeContactScore: 30,
          postureScore: 40,
          composureScore: 50,
          engagementScore: 35,
          facialExpression: "uncertain",
          gazeDirection: "down",
          bodyPosture: "slouching",
          movementStability: 50,
          timestamp
        }
      }

      const faceLandmarks = faceResults.faceLandmarks[0]
      const poseLandmarks = poseResults.landmarks[0]
      const blendshapes = faceResults.faceBlendshapes?.[0]

      // === EYE CONTACT ANALYSIS ===
      const { eyeContactScore, gazeDirection } = this.analyzeGaze(faceLandmarks)

      // === POSTURE ANALYSIS ===
      const { postureScore, bodyPosture } = this.analyzePosture(poseLandmarks)

      // === COMPOSURE/STABILITY ===
      const { composureScore, movementStability } = this.analyzeMovement(faceLandmarks, poseLandmarks)

      // === FACIAL EXPRESSION ===
      const facialExpression = this.analyzeExpression(blendshapes)

      // === OVERALL ENGAGEMENT ===
      const engagementScore = Math.round(
        eyeContactScore * 0.4 +
        postureScore * 0.3 +
        composureScore * 0.3
      )

      const result: VideoAnalysisResult = {
        eyeContactScore,
        postureScore,
        composureScore,
        engagementScore,
        facialExpression,
        gazeDirection,
        bodyPosture,
        movementStability,
        timestamp
      }

      this.lastAnalysis = result
      this.analysisHistory.push(result)
      
      // Keep only last 50 analyses (for trend detection)
      if (this.analysisHistory.length > 50) {
        this.analysisHistory.shift()
      }

      return result
    } catch (error) {
      console.error("Frame analysis error:", error)
      return null
    }
  }

  /**
   * Analyze gaze direction and eye contact
   */
  private analyzeGaze(faceLandmarks: any[]): { eyeContactScore: number, gazeDirection: "center" | "left" | "right" | "down" | "up" } {
    // Iris landmarks (468-point model includes iris)
    const leftIris = faceLandmarks[468] // Left iris center
    const rightIris = faceLandmarks[473] // Right iris center
    const noseTip = faceLandmarks[1]
    
    // Calculate gaze relative to neutral position
    const currentFaceX = noseTip.x
    const currentFaceY = noseTip.y
    
    const deltaX = currentFaceX - this.neutralFaceCenter[0]
    const deltaY = currentFaceY - this.neutralFaceCenter[1]

    // Determine gaze direction
    let gazeDirection: "center" | "left" | "right" | "down" | "up" = "center"
    if (Math.abs(deltaX) > 0.08) {
      gazeDirection = deltaX > 0 ? "right" : "left"
    } else if (Math.abs(deltaY) > 0.08) {
      gazeDirection = deltaY > 0 ? "down" : "up"
    }

    // Eye contact score: higher when looking at center
    const distanceFromCenter = Math.sqrt(deltaX * deltaX + deltaY * deltaY)
    const eyeContactScore = Math.round(Math.max(0, Math.min(100, (1 - distanceFromCenter * 5) * 100)))

    return { eyeContactScore, gazeDirection }
  }

  /**
   * Analyze body posture
   */
  private analyzePosture(poseLandmarks: any[]): { postureScore: number, bodyPosture: "upright" | "slouching" | "leaning" } {
    const leftShoulder = poseLandmarks[11]
    const rightShoulder = poseLandmarks[12]
    const nose = poseLandmarks[0]

    // Shoulder alignment
    const shoulderY = (leftShoulder.y + rightShoulder.y) / 2
    const shoulderDelta = shoulderY - this.neutralShoulderY

    // Posture classification
    let bodyPosture: "upright" | "slouching" | "leaning" = "upright"
    if (shoulderDelta > 0.05) {
      bodyPosture = "slouching" // Shoulders dropped
    } else if (Math.abs(leftShoulder.y - rightShoulder.y) > 0.08) {
      bodyPosture = "leaning" // Uneven shoulders
    }

    // Posture score
    const postureScore = Math.round(Math.max(0, Math.min(100, (1 - Math.abs(shoulderDelta) * 10) * 100)))

    return { postureScore, bodyPosture }
  }

  /**
   * Analyze movement stability (fidgeting detection)
   */
  private analyzeMovement(faceLandmarks: any[], poseLandmarks: any[]): { composureScore: number, movementStability: number } {
    if (this.analysisHistory.length < 5) {
      return { composureScore: 70, movementStability: 70 }
    }

    // Compare last 5 frames
    const recent = this.analysisHistory.slice(-5)
    
    // Calculate movement variance
    const faceMovement = recent.map((_, i) => {
      if (i === 0) return 0
      const prev = this.analysisHistory[this.analysisHistory.length - 5 + i - 1]
      const curr = recent[i]
      return Math.abs(curr.eyeContactScore - prev.eyeContactScore) + 
             Math.abs(curr.postureScore - prev.postureScore)
    })

    const avgMovement = faceMovement.reduce((a, b) => a + b, 0) / faceMovement.length
    
    // High movement = fidgeting = low composure
    const movementStability = Math.round(Math.max(0, Math.min(100, (1 - avgMovement / 50) * 100)))
    const composureScore = Math.round((movementStability * 0.6) + (this.lastAnalysis?.postureScore || 60) * 0.4)

    return { composureScore, movementStability }
  }

  /**
   * Analyze facial expression
   */
  private analyzeExpression(blendshapes: any): "confident" | "nervous" | "neutral" | "uncertain" {
    if (!blendshapes || !blendshapes.categories) return "neutral"

    // Extract key expressions from blendshapes
    const getScore = (name: string) => {
      const shape = blendshapes.categories.find((c: any) => c.categoryName === name)
      return shape ? shape.score : 0
    }

    const smileScore = getScore("mouthSmileLeft") + getScore("mouthSmileRight")
    const frownScore = getScore("mouthFrownLeft") + getScore("mouthFrownRight")
    const eyebrowUp = getScore("browInnerUp")
    const jawOpen = getScore("jawOpen")

    // Classify expression
    if (smileScore > 0.3) return "confident"
    if (frownScore > 0.2 || eyebrowUp > 0.4) return "nervous"
    if (jawOpen > 0.5) return "uncertain"
    return "neutral"
  }

  /**
   * Get aggregated analysis over time
   */
  getAggregatedAnalysis(): VideoAnalysisResult | null {
    if (this.analysisHistory.length === 0) return null

    const avg = (arr: number[]) => arr.reduce((a, b) => a + b, 0) / arr.length

    return {
      eyeContactScore: Math.round(avg(this.analysisHistory.map(a => a.eyeContactScore))),
      postureScore: Math.round(avg(this.analysisHistory.map(a => a.postureScore))),
      composureScore: Math.round(avg(this.analysisHistory.map(a => a.composureScore))),
      engagementScore: Math.round(avg(this.analysisHistory.map(a => a.engagementScore))),
      facialExpression: this.lastAnalysis?.facialExpression || "neutral",
      gazeDirection: this.lastAnalysis?.gazeDirection || "center",
      bodyPosture: this.lastAnalysis?.bodyPosture || "upright",
      movementStability: Math.round(avg(this.analysisHistory.map(a => a.movementStability))),
      timestamp: Date.now()
    }
  }

  reset() {
    this.analysisHistory = []
    this.lastAnalysis = null
    this.calibrationFrames = []
    this.isCalibrated = false
  }
}

// Singleton instance
export const videoAnalyzer = new VideoAnalyzer()
